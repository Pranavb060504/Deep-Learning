{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "args:\n",
    "    n: number of points to generate\n",
    "    xdata: generated X points\n",
    "    ydata: generated Y points\n",
    "\n",
    "    -> For each point (X,Y) ,first randomizes the length of input, then selects 2 indices where x2=1 and appends to a list  \n",
    "\"\"\"\n",
    "class Dataset:\n",
    "    def __init__(self,nop):\n",
    "        self.n=nop\n",
    "        self.xdata=[]\n",
    "        self.ydata=[]\n",
    "    \n",
    "    def create_data(self):\n",
    "        for i in range(self.n):\n",
    "            leng=np.random.randint(4,10)\n",
    "            a=np.random.randint(0,leng)\n",
    "            b=np.random.randint(0,leng)\n",
    "            while(a==b):\n",
    "                b=np.random.randint(0,leng)\n",
    "            tx=[]\n",
    "            sum=0\n",
    "            for i in range(leng):\n",
    "                if(i==a or i==b):\n",
    "                    x=torch.tensor([np.random.random(),1])\n",
    "                    sum=sum+x[0]\n",
    "                else:\n",
    "                    x=torch.tensor([np.random.random(),0])\n",
    "                tx.append(x)\n",
    "            self.xdata.append(tx)\n",
    "            self.ydata.append(torch.tensor(sum))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    layer: number of layers \n",
    "    k: dimension of the hidden state\n",
    "    U: matrix to multiply with input\n",
    "    V: matrix to multiply with hidden state \n",
    "    W: matrix for getting output\n",
    "    bh: bias for hidden state\n",
    "    by:  bias for output \n",
    "\n",
    "    forward_pass-> function which takes X and outputs predicted Y\n",
    "    tavgloss-> takes a test dataset and returns average loss(between predicted y and actual y) for all test data po\n",
    "\"\"\"\n",
    "class Elmon_RNN:\n",
    "    def __init__(self,k):\n",
    "        self.layer=1\n",
    "        self.k=k\n",
    "        self.U=torch.rand(k,2,requires_grad=True)\n",
    "        self.V=torch.rand(k,k,requires_grad=True)\n",
    "        self.W=torch.rand(1,k,requires_grad=True)\n",
    "        self.bh=torch.rand(k,requires_grad=True)\n",
    "        self.by=torch.rand(1,requires_grad=True)\n",
    "    \n",
    "    def forward_pass(self,X):\n",
    "        h=[torch.zeros(self.k)]\n",
    "        for i in range(len(X)):\n",
    "            l1=torch.tanh(torch.matmul(self.U,X[i])+torch.matmul(self.V,h[i])+self.bh)\n",
    "            h.append(l1)\n",
    "        return torch.matmul(self.W,h[-1])+self.by\n",
    "    \n",
    "    def tavgloss(self,test_dataset):\n",
    "        loss=torch.nn.MSELoss()\n",
    "        print(test_dataset.xdata)\n",
    "        score=0\n",
    "        for i in range(len(test_dataset.xdata)):\n",
    "            y_pred=self.forward_pass(test_dataset.xdata[i])\n",
    "            y_target=test_dataset.ydata[i]\n",
    "            score+=loss(y_pred,y_target).item()\n",
    "        return score/len(test_dataset.xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"creating test dataset of 5500 points\"\n",
    "data=Dataset(5500)\n",
    "data.create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Initializing an Elmon_RNN model and training it with the training data \n",
    "generated above using l.backward and optimizer.step  \n",
    "\"\"\"\n",
    "model=Elmon_RNN(8)\n",
    "learning_rate=0.01\n",
    "loss=torch.nn.MSELoss()\n",
    "print(model.U)\n",
    "optimizer=torch.optim.Adam([{\"params\":[model.U,model.V,model.W,model.bh,model.by]}],lr=learning_rate)\n",
    "loss1=[]\n",
    "n_iters=20\n",
    "for epoch in range(n_iters):\n",
    "    for pts in range(len(data.xdata)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=model.forward_pass(data.xdata[pts])\n",
    "        l=loss(y_pred,data.ydata[pts])\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    loss1.append(model.tavgloss(data))\n",
    "print(model.U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating a test data set to calculate avgloss by trained model\"\"\"\n",
    "test_data=Dataset(1000)\n",
    "test_data.create_data()\n",
    "model.tavgloss(test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Making classes for the gates used in LSTM\n",
    "\"\"\"\n",
    "class forget_gate:\n",
    "    def __init__(self,k):\n",
    "        self.Wf=torch.rand(k,k,requires_grad=True)\n",
    "        self.Uf=torch.rand(k,2,requires_grad=True)\n",
    "        self.bf=torch.rand(k,requires_grad=True)\n",
    "    def forget(self,h,x):\n",
    "        return torch.sigmoid(torch.matmul(self.Wf,h)+torch.matmul(self.Uf,x)+self.bf)\n",
    "\n",
    "class read_gate:\n",
    "    def __init__(self,k):\n",
    "        self.Wo=torch.rand(k,k,requires_grad=True)\n",
    "        self.Uo=torch.rand(k,2,requires_grad=True)\n",
    "        self.bo=torch.rand(k,requires_grad=True)\n",
    "    def read(self,h,x):\n",
    "        return torch.sigmoid(torch.matmul(self.Wo,h)+torch.matmul(self.Uo,x)+self.bo)\n",
    "\n",
    "class write_gate:\n",
    "    def __init__(self,k):\n",
    "        self.Wi=torch.rand(k,k,requires_grad=True)\n",
    "        self.Ui=torch.rand(k,2,requires_grad=True)\n",
    "        self.bi=torch.rand(k,requires_grad=True)\n",
    "    def write(self,h,x):\n",
    "        return torch.sigmoid(torch.matmul(self.Wi,h)+torch.matmul(self.Ui,x)+self.bi)\n",
    "    \n",
    "ft=forget_gate(8)\n",
    "ot=read_gate(8)\n",
    "it=write_gate(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    layer: number of layers \n",
    "    k: dimension of the hidden state\n",
    "    U: matrix to multiply with input\n",
    "    V: matrix to multiply with hidden state \n",
    "    W: matrix for getting output\n",
    "    bh: bias for hidden state\n",
    "    ft: forget gate of the LSTM\n",
    "    ot: read gate of the LSTM\n",
    "    it: write gate of the LSTM\n",
    "\n",
    "    forward_pass-> function which takes X and outputs predicted Y\n",
    "    tavgloss-> takes a test dataset and returns average loss(between predicted y and actual y) for all test data points\n",
    "\"\"\"\n",
    "class LSTM:\n",
    "    def __init__(self,k,ft,ot,it):\n",
    "        self.layer=1\n",
    "        self.k=k\n",
    "        self.U=torch.rand(k,2,requires_grad=True)\n",
    "        self.V=torch.rand(k,k,requires_grad=True)\n",
    "        self.W=torch.rand(1,k,requires_grad=True)\n",
    "        self.bh=torch.rand(k,requires_grad=True)\n",
    "        self.ft=ft\n",
    "        self.ot=ot\n",
    "        self.it=it\n",
    "    \n",
    "    def forward_pass(self,X):\n",
    "        s_t=[]\n",
    "        h=[torch.zeros(self.k)]\n",
    "        s=[torch.zeros(self.k)]\n",
    "        for i in range(len(X)):\n",
    "            l1=torch.sigmoid(torch.matmul(self.U,X[i])+torch.matmul(self.V,h[i])+self.bh)\n",
    "            s_t.append(l1)\n",
    "            s.append(self.ft.forget(h[i],X[i])*s[i]+self.it.write(h[i],X[i])*s_t[i])\n",
    "            h.append(self.ot.read(h[i],X[i])*torch.sigmoid(s[i]))\n",
    "        return s[-1]\n",
    "    \n",
    "    def tavgloss(self,test_dataset):\n",
    "        loss=torch.nn.MSELoss()\n",
    "        print(test_dataset.xdata)\n",
    "        score=0\n",
    "        for i in range(len(test_dataset.xdata)):\n",
    "            y_pred=self.forward_pass(test_dataset.xdata[i])\n",
    "            y_target=test_dataset.ydata[i]\n",
    "            score+=loss(y_pred,y_target).item()\n",
    "        return score/len(test_dataset.xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Initializing an LSTM model and training it with the training data \n",
    "generated above using l.backward and optimizer.step  also storing avgloss after each epoch in a list\n",
    "\"\"\"\n",
    "model2=LSTM(8,ft,ot,it)\n",
    "learning_rate=0.01\n",
    "loss=torch.nn.MSELoss()\n",
    "print(model.U)\n",
    "optimizer=torch.optim.Adam([{\"params\":[model2.U,model2.V,model2.W,model2.bh,model2.ft.Wf,model2.ft.Uf,model2.ot.Wo,model2.ot.Uo,model2.it.Wi,model2.it.Ui,model2.ot.bo,model2.it.bi,model2.ft.bf]}],lr=learning_rate)\n",
    "n_iters=20\n",
    "loss2=[]\n",
    "for epoch in range(n_iters):\n",
    "    for pts in range(len(data.xdata)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=model2.forward_pass(data.xdata[pts])\n",
    "        l=loss(y_pred,data.ydata[pts])\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    loss2.append(model2.tavgloss(data))\n",
    "print(model.U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "avgloss on testdata by trained model\n",
    "\"\"\"\n",
    "model2.tavgloss(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Making classes for the gates used in GRU\n",
    "\"\"\"\n",
    "class z_gate:\n",
    "     def __init__(self,k):\n",
    "        self.Wz=torch.rand(k,k,requires_grad=True)\n",
    "        self.Uz=torch.rand(k,2,requires_grad=True)\n",
    "        self.bz=torch.rand(k,requires_grad=True)\n",
    "     def zfun(self,h,x):\n",
    "        return torch.sigmoid(torch.matmul(self.Wz,h)+torch.matmul(self.Uz,x)+self.bz)\n",
    "\n",
    "class r_gate:\n",
    "     def __init__(self,k):\n",
    "        self.Wr=torch.rand(k,k,requires_grad=True)\n",
    "        self.Ur=torch.rand(k,2,requires_grad=True)\n",
    "        self.br=torch.rand(k,requires_grad=True)\n",
    "     def rfun(self,h,x):\n",
    "        return torch.sigmoid(torch.matmul(self.Wr,h)+torch.matmul(self.Ur,x)+self.br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    layer: number of layers \n",
    "    k: dimension of the hidden state\n",
    "    U: matrix to multiply with input\n",
    "    V: matrix to multiply with hidden state \n",
    "    W: matrix for getting output\n",
    "    bh: bias for hidden state\n",
    "    zt: z gate of GRU\n",
    "    rt: r gate of GRU\n",
    "\n",
    "    forward_pass-> function which takes X and outputs predicted Y\n",
    "    tavgloss-> takes a test dataset and returns average loss(between predicted y and actual y) for all test data points\n",
    "\"\"\"\n",
    "class GRU:\n",
    "   def __init__(self,k,zt,rt):\n",
    "        self.layer=1\n",
    "        self.k=k\n",
    "        self.U=torch.rand(k,2,requires_grad=True)\n",
    "        self.V=torch.rand(k,k,requires_grad=True)\n",
    "        self.W=torch.rand(1,k,requires_grad=True)\n",
    "        self.bh=torch.rand(k,requires_grad=True) \n",
    "        self.zt=zt\n",
    "        self.rt=rt\n",
    "\n",
    "   def forward_pass(self,X):\n",
    "       h=[torch.zeros(self.k)]\n",
    "       h_o=[]\n",
    "       for i in range(len(X)):\n",
    "           z_t=self.zt.zfun(h[i],X[i])\n",
    "           r_t=self.rt.rfun(h[i],X[i])\n",
    "           h_o.append(torch.tanh(torch.matmul(self.U,X[i])+torch.matmul(self.V,h[i]*r_t)+self.bh))\n",
    "           h.append((1-z_t)*h[i]+z_t*h_o[i])\n",
    "       return h[-1]\n",
    "   \n",
    "   def tavgloss(self,test_dataset):\n",
    "        loss=torch.nn.MSELoss()\n",
    "        print(test_dataset.xdata)\n",
    "        score=0\n",
    "        for i in range(len(test_dataset.xdata)):\n",
    "            y_pred=self.forward_pass(test_dataset.xdata[i])\n",
    "            y_target=test_dataset.ydata[i]\n",
    "            score+=loss(y_pred,y_target).item()\n",
    "        return score/len(test_dataset.xdata)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Initializing an GRU model and training it with the training data \n",
    "generated above using l.backward and optimizer.step  also storing avgloss after each epoch in a list\n",
    "\"\"\"\n",
    "zt=z_gate(8)\n",
    "rt=r_gate(8)\n",
    "model3=GRU(8,zt,rt)\n",
    "learning_rate=0.01\n",
    "loss=torch.nn.MSELoss()\n",
    "print(model.U)\n",
    "optimizer=torch.optim.Adam([{\"params\":[model3.U,model3.V,model3.W,model3.bh,model3.rt.Wr,model3.rt.Ur,model3.zt.Wz,model3.zt.Uz,model3.zt.bz,model3.rt.br]}],lr=learning_rate)\n",
    "n_iters=20\n",
    "loss3=[]\n",
    "for epoch in range(n_iters):\n",
    "    for pts in range(len(data.xdata)):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred=model3.forward_pass(data.xdata[pts])\n",
    "        l=loss(y_pred,data.ydata[pts])\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    loss3.append(model3.tavgloss(data))\n",
    "print(model.U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "avgloss on testdata by trained model\n",
    "\"\"\"\n",
    "model3.tavgloss(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting loss after each epoch of each model\n",
    "\"\"\"\n",
    "ax=range(n_iters)\n",
    "plt.plot(ax,loss1,label='Elmon_RNN')\n",
    "plt.plot(ax,loss2,color='r',label='LSTM')\n",
    "plt.plot(ax,loss3,color='orange',label='GRU')\n",
    "plt.xlabel('number of epochs')\n",
    "plt.ylabel('loss after each epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
